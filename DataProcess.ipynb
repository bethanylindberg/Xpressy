{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies to Visualize the model\n",
    "%matplotlib inline\n",
    "from IPython.display import Image, SVG\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(0)\n",
    "# Filepaths, numpy, and Tensorflow\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# Sklearn scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Keras\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense\n",
    "#Images\n",
    "from PIL import Image\n",
    "import glob\n",
    "import cv2\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "def getImagesDf(emotion):\n",
    "    path = f'samples/{emotion}/'\n",
    "    df = pd.DataFrame()\n",
    "    x=0\n",
    "    for filename in glob.glob(f\"{path}*.jpg\"):\n",
    "        df.loc[x,\"filename\"] = filename.split('\\\\')[0]+'/'\n",
    "        df.loc[x,\"image\"] = filename.split(\"\\\\\")[1]\n",
    "        df.loc[x,\"emotion\"] = emotion\n",
    "\n",
    "        x+=1\n",
    "    for filename in glob.glob(f\"{path}*.png\"):\n",
    "        df.loc[x,\"filename\"] = filename.split('\\\\')[0]+'/'\n",
    "        df.loc[x,\"image\"] = filename.split(\"\\\\\")[1]\n",
    "        df.loc[x,\"emotion\"] = emotion\n",
    "        x+=1\n",
    "\n",
    "    return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>filename</th>\n",
       "      <th>image</th>\n",
       "      <th>image_process</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>surprise</td>\n",
       "      <td>samples/surprise/</td>\n",
       "      <td>ray_surprise_744.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>surprise</td>\n",
       "      <td>samples/surprise/</td>\n",
       "      <td>ray_surprise_745.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>surprise</td>\n",
       "      <td>samples/surprise/</td>\n",
       "      <td>ray_surprise_746.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>surprise</td>\n",
       "      <td>samples/surprise/</td>\n",
       "      <td>ray_surprise_747.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>surprise</td>\n",
       "      <td>samples/surprise/</td>\n",
       "      <td>ray_surprise_748.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      emotion           filename                 image image_process label\n",
       "580  surprise  samples/surprise/  ray_surprise_744.png           NaN     7\n",
       "581  surprise  samples/surprise/  ray_surprise_745.png           NaN     7\n",
       "582  surprise  samples/surprise/  ray_surprise_746.png           NaN     7\n",
       "583  surprise  samples/surprise/  ray_surprise_747.png           NaN     7\n",
       "584  surprise  samples/surprise/  ray_surprise_748.png           NaN     7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict \n",
    "emotions_get = OrderedDict([('angry',1),('disgust', 2),('fear', 3),('happy', 4),('neutral', 5),('sad', 6),('surprise', 7)]) \n",
    "emotions = pd.DataFrame(columns=[\"filename\",\"image\",\"emotion\",\"label\",\"image_process\"])\n",
    "\n",
    "for k,v in emotions_get.items():\n",
    "    df = getImagesDf(k)\n",
    "    df[\"label\"] = v\n",
    "    emotions = emotions.append(df,ignore_index=True,sort=True)\n",
    "emotions.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageProcess(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        face_clip = img[y:y+h, x:x+w]  #cropping the face in image\n",
    "        img = cv2.resize(face_clip, (350, 350))  #resizing image\n",
    "    return img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples/angry/malcolm_anger_841.png\n",
      "samples/angry/malcolm_anger_843.png\n",
      "samples/angry/malcolm_anger_844.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>filename</th>\n",
       "      <th>image</th>\n",
       "      <th>image_process</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angry</td>\n",
       "      <td>samples/angry/</td>\n",
       "      <td>377.jpg</td>\n",
       "      <td>[[[122, 122, 122], [159, 159, 159], [135, 135,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>angry</td>\n",
       "      <td>samples/angry/</td>\n",
       "      <td>387.jpg</td>\n",
       "      <td>[[[1, 1, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>angry</td>\n",
       "      <td>samples/angry/</td>\n",
       "      <td>393.jpg</td>\n",
       "      <td>[[[42, 42, 42], [57, 57, 57], [53, 53, 53], [5...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>angry</td>\n",
       "      <td>samples/angry/</td>\n",
       "      <td>404.jpg</td>\n",
       "      <td>[[[255, 255, 255], [253, 253, 253], [255, 255,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>angry</td>\n",
       "      <td>samples/angry/</td>\n",
       "      <td>409.jpg</td>\n",
       "      <td>[[[153, 153, 153], [170, 170, 170], [169, 169,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  emotion        filename    image  \\\n",
       "0   angry  samples/angry/  377.jpg   \n",
       "1   angry  samples/angry/  387.jpg   \n",
       "2   angry  samples/angry/  393.jpg   \n",
       "3   angry  samples/angry/  404.jpg   \n",
       "4   angry  samples/angry/  409.jpg   \n",
       "\n",
       "                                       image_process label  \n",
       "0  [[[122, 122, 122], [159, 159, 159], [135, 135,...     1  \n",
       "1  [[[1, 1, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...     1  \n",
       "2  [[[42, 42, 42], [57, 57, 57], [53, 53, 53], [5...     1  \n",
       "3  [[[255, 255, 255], [253, 253, 253], [255, 255,...     1  \n",
       "4  [[[153, 153, 153], [170, 170, 170], [169, 169,...     1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "for i, row in enumerate(emotions.itertuples()):\n",
    "    try:\n",
    "        path = f\"{row.filename.strip()}{row.image.strip()}\"\n",
    "        img = imageProcess(path)\n",
    "        emotions.loc[i,\"image_process\"] = img\n",
    "        time.sleep(.2)\n",
    "    except:\n",
    "        print(path)\n",
    "        emotions.loc[i,\"image_process\"] = np.nan\n",
    "\n",
    "emotions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion          0\n",
       "filename         0\n",
       "image            0\n",
       "image_process    3\n",
       "label            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23788185b70>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Drop the null rows\n",
    "emotions = emotions.dropna()\n",
    "\n",
    "plt.imshow(emotions.image_process[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = emotions[\"image_process\"].values.reshape(-1,1)\n",
    "y = emotions[\"label\"]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions.flat_img[20].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape: (436,)\n",
      "Testing Shape: (146,)\n"
     ]
    }
   ],
   "source": [
    "# ndims = X_train.shape[1] * X_train.shape[2]\n",
    "# X_train = X_train.reshape(X_train.shape[0], ndims)\n",
    "# X_test = X_test.reshape(X_test.shape[0], ndims)\n",
    "print(\"Training Shape:\", X_train.shape)\n",
    "print(\"Testing Shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>filename</th>\n",
       "      <th>image</th>\n",
       "      <th>image_process</th>\n",
       "      <th>label</th>\n",
       "      <th>flat_img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angry</td>\n",
       "      <td>samples/angry/</td>\n",
       "      <td>377.jpg</td>\n",
       "      <td>[[[122, 122, 122], [159, 159, 159], [135, 135,...</td>\n",
       "      <td>1</td>\n",
       "      <td>[122, 122, 122, 159, 159, 159, 135, 135, 135, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>angry</td>\n",
       "      <td>samples/angry/</td>\n",
       "      <td>387.jpg</td>\n",
       "      <td>[[[1, 1, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>angry</td>\n",
       "      <td>samples/angry/</td>\n",
       "      <td>393.jpg</td>\n",
       "      <td>[[[42, 42, 42], [57, 57, 57], [53, 53, 53], [5...</td>\n",
       "      <td>1</td>\n",
       "      <td>[42, 42, 42, 57, 57, 57, 53, 53, 53, 58, 58, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>angry</td>\n",
       "      <td>samples/angry/</td>\n",
       "      <td>404.jpg</td>\n",
       "      <td>[[[255, 255, 255], [253, 253, 253], [255, 255,...</td>\n",
       "      <td>1</td>\n",
       "      <td>[255, 255, 255, 253, 253, 253, 255, 255, 255, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>angry</td>\n",
       "      <td>samples/angry/</td>\n",
       "      <td>409.jpg</td>\n",
       "      <td>[[[153, 153, 153], [170, 170, 170], [169, 169,...</td>\n",
       "      <td>1</td>\n",
       "      <td>[153, 153, 153, 170, 170, 170, 169, 169, 169, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  emotion        filename    image  \\\n",
       "0   angry  samples/angry/  377.jpg   \n",
       "1   angry  samples/angry/  387.jpg   \n",
       "2   angry  samples/angry/  393.jpg   \n",
       "3   angry  samples/angry/  404.jpg   \n",
       "4   angry  samples/angry/  409.jpg   \n",
       "\n",
       "                                       image_process label  \\\n",
       "0  [[[122, 122, 122], [159, 159, 159], [135, 135,...     1   \n",
       "1  [[[1, 1, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...     1   \n",
       "2  [[[42, 42, 42], [57, 57, 57], [53, 53, 53], [5...     1   \n",
       "3  [[[255, 255, 255], [253, 253, 253], [255, 255,...     1   \n",
       "4  [[[153, 153, 153], [170, 170, 170], [169, 169,...     1   \n",
       "\n",
       "                                            flat_img  \n",
       "0  [122, 122, 122, 159, 159, 159, 135, 135, 135, ...  \n",
       "1  [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [42, 42, 42, 57, 57, 57, 53, 53, 53, 58, 58, 5...  \n",
       "3  [255, 255, 255, 253, 253, 253, 255, 255, 255, ...  \n",
       "4  [153, 153, 153, 170, 170, 170, 169, 169, 169, ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions[\"flat_img\"] = emotions[\"image_process\"].map(lambda x: x.reshape(-1))\n",
    "emotions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = emotions[\"flat_img\"]\n",
    "y = emotions[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty sequential model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0717 11:07:57.960280  5052 deprecation.py:506] From C:\\Users\\bethf\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Error converting shape to a TensorShape: int() argument must be a string, a bytes-like object or a number, not 'tuple'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mmake_shape\u001b[1;34m(v, arg_name)\u001b[0m\n\u001b[0;32m    145\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36mas_shape\u001b[1;34m(shape)\u001b[0m\n\u001b[0;32m   1203\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1204\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dims)\u001b[0m\n\u001b[0;32m    773\u001b[0m         \u001b[1;31m# Got a list of dimensions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 774\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    773\u001b[0m         \u001b[1;31m# Got a list of dimensions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 774\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36mas_dimension\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    715\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mDimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m       if (not isinstance(value, compat.bytes_or_text_types) and\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'tuple'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-9be9d662d07e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Add the first layer where the input dimensions are the 784 pixel values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# We can also choose our activation function. `relu` is a common\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    168\u001b[0m           \u001b[1;31m# Instantiate an input layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m           x = input_layer.Input(\n\u001b[1;32m--> 170\u001b[1;33m               batch_shape=batch_shape, dtype=dtype, name=layer.name + '_input')\n\u001b[0m\u001b[0;32m    171\u001b[0m           \u001b[1;31m# This will build the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m           \u001b[1;31m# and create the node connecting the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_layer.py\u001b[0m in \u001b[0;36mInput\u001b[1;34m(shape, batch_size, name, dtype, sparse, tensor, **kwargs)\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[0msparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m         input_tensor=tensor)\n\u001b[0m\u001b[0;32m    249\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m     input_layer = InputLayer(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_shape, batch_size, dtype, input_tensor, sparse, name, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m               \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_input_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m               \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m               name=self.name)\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_placeholder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mplaceholder\u001b[1;34m(shape, ndim, dtype, sparse, name)\u001b[0m\n\u001b[0;32m    996\u001b[0m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse_placeholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    997\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 998\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    999\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[1;34m(dtype, shape, name)\u001b[0m\n\u001b[0;32m   2141\u001b[0m                        \"eager execution.\")\n\u001b[0;32m   2142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2143\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[1;34m(dtype, shape, name)\u001b[0m\n\u001b[0;32m   7397\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7398\u001b[0m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7399\u001b[1;33m   \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"shape\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7400\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m   7401\u001b[0m         \"Placeholder\", dtype=dtype, shape=shape, name=name)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mmake_shape\u001b[1;34m(v, arg_name)\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Error converting %s to a TensorShape: %s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0marg_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     raise ValueError(\"Error converting %s to a TensorShape: %s.\" % (arg_name,\n",
      "\u001b[1;31mTypeError\u001b[0m: Error converting shape to a TensorShape: int() argument must be a string, a bytes-like object or a number, not 'tuple'."
     ]
    }
   ],
   "source": [
    "# Add the first layer where the input dimensions are the 784 pixel values\n",
    "# We can also choose our activation function. `relu` is a common\n",
    "model.add(Dense(100, activation='relu', input_dim=X_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
